# Principal-Component-Analysis

لعنة الأبعاد Curse of Dimensionality
في مشروع ML  هتقابلك مشكلة الأبعاد ركز بس معايا 
- كبشر عندنا قدرة تخيل بعد واحد أو بعدين أقصى حاجة ممكن نرسمها هي 3 أبعاد وكلما زادت كلما بقى صعب اني استعرض البيانات اللي عندي بصريا 
- غير انه كلما زادت الأبعاد (نقصد بها attributes or fields in the structured dataset) كلما احتجنا لمعالجات وتكاليف كتير جدا 
- كلما زادت الابعاد قلت الدقة accuracy بتاعتك واحتمال كبير جدا يعمل overfitting 
ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ
ازاي نحل المشكلة دي اكيد هنعمل تقليل الابعاد Dimensionality Reduction
تقليل الأبعاد مهمتها انها تنقلك من بعد عالي لبعد أقل بس من غير فقد في المعلومات انك تضغطها بس متخسرش منها حاجة لانك لو خسرت حاجة فده معناه فشل النموذج بشكل مباشر
ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ
ايه فايدة تقليل الأبعاد Dimensionality Reduction
- هتقلل التكاليف المادية اللي هتحتاجها وكمان هيقلل وقت التدريب معاك جدا 
- هيخليك قادر تشوف البيانات بصريا وبكده لو متمكن هتقدر تفهم كويس بياناتك
- بيشيل redundancy الموجودة في data اللي عندك ويخلي قدامك relevant information بس 
ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ
ازاي ممكن تعمل  تقليل الأبعاد Dimensionality Reduction
عندنا طرق كتير جدا عشان نعملها ومنها مثلا 
Principal Component Analysis (PCA)
Linear Discriminant Analysis (LDA)
Generalized Discriminant Analysis (GDA)
ــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ
خلينا نركز على واحدة بس منهم وهي PCA أعظم اختراع وأعظم طريقة اخترعها البشر يوما فيه عمالقة كتير جدا بيأرخوا للذكاء الاصظناعي من يوم ما قدرنا نستخدمها في عملياتنا 
- أول حاجة لازم تعملها في الخطوات هي Standardization لل Dataset  بتاعتك هتقولي ليه هقولك لانه PCA حساسة لل high variance الموجود فيها 
_ التانية هتحسب مصفوفة التغاير covariance matrix 
- التالته هتحسب Eigenvalues and Eigenvectors باستخدام covariance matrix  
- الرابعة هترتبهم Eigenvalues and Eigenvectors ترتيب تنازلي والمتجه الأعلى قيمة هو أهم واحد بالنسبالك وهو المكون الرئيسي ما هو التقنية اسمها تحليل المكون الرئيس 
- الخامسه هتعمل Transform للمصفوفة الأصلية وده من خلال ضربها في أول eigenvectors اخترتهم فوق 
لو مفهمتش يبقى العيب منك مش مني بقى (الجملة للهزار وده تنويه) 
فيه جزء تاني هو التطبيق العملي لها وهكتب الكود وانزله 
الفكرة انك حرفيا ممكن تستخدمها في أي حاجة حتى قبل ما تختار الالجورزم بتاعتك لانها زي ما قولنا فوق
